{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import itertools\n",
    "import logging\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as ks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gnn_teacher_student.main import StudentTeacherExplanationAnalysis\n",
    "from gnn_teacher_student.training import ReferenceStudentTraining, ExplanationLoss, ExplanationPreTraining\n",
    "from gnn_teacher_student.students import StudentTemplate, GeneralAttentionStudent\n",
    "from gnn_teacher_student.layers import (NodeImportanceSubNetwork,\n",
    "                                        EdgeImportanceSubNetwork,\n",
    "                                        ConvolutionalSubNetwork)\n",
    "from gnn_teacher_student.data import generate_color_pairs_dataset\n",
    "from gnn_teacher_student.visualization import (draw_colors_graph,\n",
    "                                               draw_graph_node_importances,\n",
    "                                               draw_graph_edge_importances)\n",
    "\n",
    "\n",
    "# Disabling all kinds of warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn = lambda *args, **kwargs: 0\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Enable eager execution\n",
    "tf.compat.v1.enable_eager_execution()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET SUMMARY\n",
      "==============================================================\n",
      "Number of Graphs: 2000\n",
      "--------------------------------------------------------------\n",
      "Min Graph Size: 5\n",
      "Max Graph Size: 30\n",
      "--------------------------------------------------------------\n",
      "Number of distinct Colors: 3\n",
      "Color Distribution:\n",
      "  Color (0.0, 0.0, 1.0): 11923 total nodes (32.9%)\n",
      "  Color (1.0, 0.0, 0.0): 12107 total nodes (33.4%)\n",
      "  Color (0.0, 1.0, 0.0): 12235 total nodes (33.7%)\n",
      "--------------------------------------------------------------\n",
      "GT Labels: Total number of color pairs\n",
      "Label Distribution:\n",
      "   Label 1: 319   (16.0%)\n",
      "   Label 2: 457   (22.9%)\n",
      "   Label 3: 433   (21.6%)\n",
      "   Label 4: 332   (16.6%)\n",
      "   Label 5: 226   (11.3%)\n",
      "   Label 6: 124   (6.2%)\n",
      "   Label 7: 65    (3.2%)\n",
      "   Label 8: 32    (1.6%)\n",
      "   Label 9: 7     (0.4%)\n",
      "   Label 10: 3     (0.1%)\n",
      "   Label 12: 2     (0.1%)\n"
     ]
    }
   ],
   "source": [
    "# ~ Generating the dataset\n",
    "\n",
    "dataset = generate_color_pairs_dataset(\n",
    "    length=2000,\n",
    "    node_count_cb=lambda: random.randint(5, 30),\n",
    "    additional_edge_count_cb=lambda: random.randint(1, 3),\n",
    "    colors=[\n",
    "        (1, 0, 0),  # red\n",
    "        (0, 1, 0),  # green\n",
    "        (0, 0, 1),  # blue\n",
    "        #(1, 1, 0),  # yellow\n",
    "        #(1, 0, 1),  # magenta\n",
    "    ],\n",
    "    exclude_empty=True\n",
    ")\n",
    "\n",
    "# ~ Printing information about dataset\n",
    "graph_sizes = [len(g['node_indices']) for g in dataset]\n",
    "graph_labels = [int(g['graph_labels']) for g in dataset]\n",
    "graph_labels_counter = Counter(graph_labels)\n",
    "\n",
    "graph_colors = [[tuple(color) for color in g['node_attributes']] for g in dataset]\n",
    "graph_colors_combined = list(itertools.chain(*graph_colors))\n",
    "graph_colors_counter = Counter(graph_colors_combined)\n",
    "\n",
    "print('DATASET SUMMARY')\n",
    "print('==============================================================')\n",
    "print(f'Number of Graphs: {len(dataset)}')\n",
    "print('--------------------------------------------------------------')\n",
    "print(f'Min Graph Size: {min(graph_sizes)}')\n",
    "print(f'Max Graph Size: {max(graph_sizes)}')\n",
    "print('--------------------------------------------------------------')\n",
    "print(f'Number of distinct Colors: {len(graph_colors_counter)}')\n",
    "print(f'Color Distribution:')\n",
    "for color, count in graph_colors_counter.items():\n",
    "    percentage = count / len(graph_colors_combined)\n",
    "    print(f'  Color {color}: {count} total nodes ({percentage*100:.1f}%)')\n",
    "print('--------------------------------------------------------------')\n",
    "print('GT Labels: Total number of color pairs')\n",
    "print('Label Distribution:')\n",
    "for label, count in sorted(graph_labels_counter.items(), key=lambda i: i[0]):\n",
    "    percentage = count / len(graph_labels)\n",
    "    print(f'   Label {label}: {count:<5} ({percentage*100:.1f}%)')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def create_node_importance_network():\n",
    "    return NodeImportanceSubNetwork(\n",
    "        unitss=[3],\n",
    "        activation='tanh',\n",
    "        use_softmax=True,\n",
    "        use_bias=False\n",
    "    )\n",
    "\n",
    "def create_edge_importance_network():\n",
    "    return EdgeImportanceSubNetwork(\n",
    "        unitss=[3],\n",
    "        activation='tanh',\n",
    "        use_softmax=True,\n",
    "        use_bias=False\n",
    "    )\n",
    "\n",
    "def create_prediction_network():\n",
    "    return ConvolutionalSubNetwork(\n",
    "        unitss=[2, 2],\n",
    "        use_bias=False,\n",
    "        activation='tanh'\n",
    "    )\n",
    "\n",
    "attention_student = StudentTemplate(\n",
    "    student_name='gas',\n",
    "    student_class=GeneralAttentionStudent,\n",
    "    lay_node_importance_cb=create_node_importance_network,\n",
    "    lay_edge_importance_cb=create_edge_importance_network,\n",
    "    lay_prediction_cb=create_prediction_network\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting student training \"gas:exp\" [LOSS:] prediction=\"mean_squared_error*<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\" node_importance=\"explanation_loss*<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\" edge_importance=\"explanation_loss*<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\" [BATCHING:] batch_size=200 supports_batching=\"True\" [MODEL:] parameters=57 [TRAINING:] epochs=5000 optimizer=Adam dataset_size=6) \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5000\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "student_teacher_analysis = StudentTeacherExplanationAnalysis(\n",
    "    student_template=attention_student,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    optimizer=ks.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    prediction_metric=ks.metrics.MeanSquaredError(),\n",
    "    explanation_metric=ks.metrics.MeanAbsoluteError()\n",
    ")\n",
    "\n",
    "student_teacher_analysis.logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "_dataset = {field: [g[field] for g in dataset] for field in dataset[0].keys()}\n",
    "\n",
    "explanation_pre_training = ExplanationPreTraining(\n",
    "    epochs=int(0.3 * EPOCHS),\n",
    "    loss=[\n",
    "        ks.losses.MeanSquaredError(),\n",
    "        ExplanationLoss(loss_function=ks.losses.mean_absolute_error),\n",
    "        ExplanationLoss(loss_function=ks.losses.mean_absolute_error)\n",
    "    ],\n",
    "    post_weights=[1, 0.3, 0.3],\n",
    "    lock_explanation=False\n",
    ")\n",
    "\n",
    "reference_student_training = ReferenceStudentTraining()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    results = student_teacher_analysis.fit(\n",
    "        dataset=_dataset,\n",
    "        train_split=0.7,\n",
    "        variant_kwargs={\n",
    "            'exp': explanation_pre_training(),\n",
    "            'ref': reference_student_training()\n",
    "        },\n",
    "        log_progress=int(0.2 * EPOCHS)\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}